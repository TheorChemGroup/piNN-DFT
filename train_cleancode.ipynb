{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np    \n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from SVWN3 import f_svwn3\n",
    "import csv\n",
    "import copy\n",
    "from NN_models import NN_2_256, NN_8_256, NN_8_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref(x, y):\n",
    "    ''' \n",
    "    returns reference energies for points of a reaction grid from Reference_data.csv\n",
    "    '''\n",
    "    hartree2kcal = 627.5095\n",
    "    with open(\"Reference_data.csv\", newline='', encoding='cp1251') as csvfile:\n",
    "        ref_file = csv.reader(csvfile, delimiter=\",\")\n",
    "        k = 1\n",
    "        if y == 391:\n",
    "            k = hartree2kcal\n",
    "        ref = []\n",
    "        for n, i in enumerate(ref_file):\n",
    "            if x <= n + 1 <= y:\n",
    "                ref.append((i[0], float(i[2]) * k))\n",
    "\n",
    "        return ref\n",
    "\n",
    "def load_ref_energies():\n",
    "    '''Returns {db_name: [equation, energy]}'''\n",
    "    ref_e = { # Получение референсных энергий\n",
    "        \"MGAE109\":ref(8, 116),\n",
    "        \"IP13\":ref(155, 167),\n",
    "        \"EA13\":ref(180, 192),\n",
    "        \"PA8\":ref(195, 202),\n",
    "        \"DBH76\":ref(251, 288) + ref(291, 328),\n",
    "        \"NCCE31\":ref(331, 361),\n",
    "        \"ABDE4\":ref(206, 209),\n",
    "        # \"AE17\":ref(375, 391),\n",
    "        \"pTC13\":ref(232, 234) + ref(237, 241) + ref(244, 248)\n",
    "        } \n",
    "    return ref_e\n",
    "\n",
    "def load_component_names():\n",
    "    '''\n",
    "    Returns {db_name: {id: {'Components': [...], 'Coefficients: [...]'\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "     which is a dictionary with Components and Coefficients data about all reactions\n",
    "    '''\n",
    "    with open(\"total_dataframe_sorted_final.csv\", newline='', encoding='cp1251') as csvfile:\n",
    "        ref_file = csv.reader(csvfile, delimiter=\",\")\n",
    "        ref = dict()\n",
    "        current_database = None\n",
    "        \n",
    "        for n, line in enumerate(ref_file):\n",
    "            line = np.array(line)\n",
    "            if n == 0:\n",
    "                components = np.array(line)\n",
    "            else:\n",
    "                reaction_id = int(line[0])\n",
    "                reaction_database = line[1]\n",
    "                reaction_component_num = np.nonzero(list(map(float, line[2:])))[0] + 2\n",
    "                if reaction_database in ref:\n",
    "                    ref[reaction_database][reaction_id] = {'Components': components[reaction_component_num], 'Coefficients': line[reaction_component_num]}\n",
    "                else: \n",
    "                    ref[reaction_database] = {reaction_id: {'Components': components[reaction_component_num], 'Coefficients': line[reaction_component_num]}}\n",
    "        return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compounds_coefs_energy_v2(reactions, energies):\n",
    "    '''Returns {id: \n",
    "                    {'Components': [...], 'Coefficients: [...]', 'Energy: float', Database: str\n",
    "                                }\n",
    "                            }\n",
    "    which is a dictionaty from load_component_names with Energy information added\n",
    "    '''\n",
    "    data_final = dict()\n",
    "    i = 0\n",
    "    databases = load_ref_energies().keys()\n",
    "    for database in databases:\n",
    "        data = reactions[database]\n",
    "        for reaction in data:\n",
    "            data[reaction]['Energy'] = energies[database][reaction][1]\n",
    "            data_final[i] = {'Database': database,\n",
    "                         'Components': reactions[database][reaction]['Components'],\n",
    "                         'Coefficients': reactions[database][reaction]['Coefficients'].astype(np.float64),\n",
    "                         'Energy': energies[database][reaction][1]\n",
    "            \n",
    "        }\n",
    "            i += 1\n",
    "        \n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h5_names(reaction):\n",
    "    '''reaction must be from the function get_compounds_coefs_energy_v2'''\n",
    "    database_match = {\n",
    "        'MGAE109': 'mgae109',\n",
    "        'IP13': 'ip13',\n",
    "        'EA13': 'ea13',\n",
    "        'PA8': 'pa8',\n",
    "        'DBH76': 'ntbh38',\n",
    "        'NCCE31': 'ncce31',\n",
    "        'ABDE4': 'abde4',\n",
    "        'AE17': 'ae17',\n",
    "        'pTC13': 'ptc13'\n",
    "    }\n",
    "    names = []\n",
    "    for elem in reaction['Components']:\n",
    "        database = database_match[reaction['Database']]\n",
    "        names.append(f'{elem}.h5')\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reaction_info_from_h5(reaction):\n",
    "    '''\n",
    "    reaction must be from get_compounds_coefs_energy_v2\n",
    "    returns merged descriptos array X, integration weights, \n",
    "    a and b densities and indexes for backsplitting\n",
    "    \n",
    "    Adds the following information to the reaction dict using h5 files from the dataset:\n",
    "    Grid : np.array with grid descriptors (scaled with np.log1p)\n",
    "    Weights : list with integration weights of grid points\n",
    "    Densities : np.array with alpha and beta densities data for grid points\n",
    "    HF_energies : list of Total HF energy (T+V) which needs to be added to E_xc\n",
    "    backsplit_ind: list of indexes where we concatenate molecules' grids\n",
    "    '''\n",
    "    X = np.array([]) \n",
    "    backsplit_ind = []\n",
    "    HF_energies = np.array([])\n",
    "    for component_filename in get_h5_names(reaction):\n",
    "        with h5py.File(component_filename, \"r\") as f:\n",
    "            HF_energies = np.append(HF_energies, f[\"ener\"][:][0])\n",
    "            X_raw = np.array(f[\"grid\"][:])\n",
    "            if len(X) == 0:\n",
    "                X = X_raw[:, 3:-1]\n",
    "            else:\n",
    "                X = np.vstack((X, X_raw[:, 3:-1]))\n",
    "            backsplit_ind.append(len(X))\n",
    "    densities = X[:, 1:3]\n",
    "    weights = X[:,0]\n",
    "    X = np.log1p(X[:, 1:])\n",
    "\n",
    "    labels = ['Grid', 'Weights', 'Densities', 'HF_energies', 'backsplit_ind']\n",
    "    values = [X, weights, densities, HF_energies, backsplit_ind]\n",
    "    for label, value in zip(labels, values):\n",
    "        reaction[label] = value\n",
    "\n",
    "    return reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reactions_dict():\n",
    "    '''\n",
    "    Returns a dict like {reaction_id: {*reaction info}} with all info available listed below:\n",
    "    ['Database', 'Components', 'Coefficients', 'Energy', 'Grid', 'Weights', 'Densities', 'HF_energies', 'backsplit_ind']\n",
    "    '''\n",
    "    data = get_compounds_coefs_energy_v2(load_component_names(), load_ref_energies())\n",
    "    for i in data.keys():\n",
    "        data[i] = add_reaction_info_from_h5(data[i])\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_energies(reaction, constants, densities):\n",
    "    calc_reaction_data = {}\n",
    "    local_energies = np.array(list(map(f_svwn3, densities, constants)))\n",
    "    calc_reaction_data['Local_energies'] = local_energies\n",
    "    return calc_reaction_data\n",
    "\n",
    "\n",
    "def add_calc_reaction_data(reaction, calc_reaction_data):\n",
    "    calc_reaction_data['Weights'] = reaction['Weights']\n",
    "    calc_reaction_data['Densities'] = reaction['Densities']\n",
    "    return calc_reaction_data\n",
    "\n",
    "def backsplit(reaction, calc_reaction_data):\n",
    "    backsplit_ind = reaction['backsplit_ind']\n",
    "    splitted_data = dict()\n",
    "    stop = 0\n",
    "\n",
    "    for i, component in enumerate(reaction['Components']):\n",
    "        splitted_data[component] = dict()\n",
    "        start = stop\n",
    "        stop = backsplit_ind[i]\n",
    "        for elem in ('Local_energies', 'Weights', 'Densities'):\n",
    "            splitted_data[component][elem] = calc_reaction_data[elem][start:stop]\n",
    "    return splitted_data\n",
    "\n",
    "\n",
    "def integration(reaction, splitted_calc_reaction_data):\n",
    "    molecule_energies = dict()\n",
    "    for i, component in enumerate(reaction['Components']):\n",
    "        molecule_energies[component] = np.sum(splitted_calc_reaction_data[component]['Local_energies'] \\\n",
    "                                              * (splitted_calc_reaction_data[component]['Densities'][:,0] \\\n",
    "                                              + splitted_calc_reaction_data[component]['Densities'][:,1]) \\\n",
    "                                              * (splitted_calc_reaction_data[component]['Weights'])) \\\n",
    "                                              + reaction['HF_energies'][i]\n",
    "    return molecule_energies\n",
    "\n",
    "\n",
    "def get_energy_reaction(reaction, molecule_energies):\n",
    "    hartree2kcal = 627.5095\n",
    "    s = 0\n",
    "    for coef, ener in zip(reaction['Coefficients'], molecule_energies.values()):\n",
    "        s += coef*ener\n",
    "    reaction_energy_kcal = s * hartree2kcal\n",
    "    return reaction_energy_kcal\n",
    "\n",
    "\n",
    "def calculate_reaction_energy(reaction, constants):\n",
    "    calc_reaction_data = get_local_energies(reaction, constants, reaction['Densities'])\n",
    "    calc_reaction_data = add_calc_reaction_data(reaction, calc_reaction_data)\n",
    "\n",
    "    splitted_calc_reaction_data = backsplit(reaction, calc_reaction_data)\n",
    "\n",
    "    molecule_energies = integration(reaction, splitted_calc_reaction_data)\n",
    "    \n",
    "    reaction_energy_kcal = get_energy_reaction(reaction, molecule_energies)\n",
    "\n",
    "    return reaction_energy_kcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'C_mgae109.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m make_reactions_dict()\n",
      "Cell \u001b[1;32mIn [6], line 8\u001b[0m, in \u001b[0;36mmake_reactions_dict\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m data \u001b[39m=\u001b[39m get_compounds_coefs_energy_v2(load_component_names(), load_ref_energies())\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m----> 8\u001b[0m     data[i] \u001b[39m=\u001b[39m add_reaction_info_from_h5(data[i])\n\u001b[0;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "Cell \u001b[1;32mIn [5], line 18\u001b[0m, in \u001b[0;36madd_reaction_info_from_h5\u001b[1;34m(reaction)\u001b[0m\n\u001b[0;32m     16\u001b[0m HF_energies \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m component_filename \u001b[39min\u001b[39;00m get_h5_names(reaction):\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39;49mFile(component_filename, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     19\u001b[0m         HF_energies \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(HF_energies, f[\u001b[39m\"\u001b[39m\u001b[39mener\u001b[39m\u001b[39m\"\u001b[39m][:][\u001b[39m0\u001b[39m])\n\u001b[0;32m     20\u001b[0m         X_raw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(f[\u001b[39m\"\u001b[39m\u001b[39mgrid\u001b[39m\u001b[39m\"\u001b[39m][:])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\h5py\\_hl\\files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    525\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    527\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[0;32m    528\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[0;32m    529\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    530\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    531\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[0;32m    532\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 533\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\h5py\\_hl\\files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    225\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 226\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    227\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    228\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'C_mgae109.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "data = make_reactions_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(data, test_size, shuffle=False):\n",
    "    if shuffle:\n",
    "        keys = list(data.keys())\n",
    "        random.shuffle(keys)\n",
    "        for i in keys:\n",
    "            data[keys[i]] = data[i]\n",
    "\n",
    "    train, test = dict(), dict()\n",
    "    border = round(len(data.keys()) * (1 - test_size))\n",
    "    for i in range(len(data.keys())):\n",
    "        if i <= border:\n",
    "            train[i] = data[i]\n",
    "        else:\n",
    "            test[i] = data[i]\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     12\u001b[0m test_size \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[1;32m---> 14\u001b[0m data_train, data_test \u001b[39m=\u001b[39m train_split(data, \u001b[39m0.2\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m train_set \u001b[39m=\u001b[39m Dataset(data\u001b[39m=\u001b[39mdata_train)\n\u001b[0;32m     18\u001b[0m train_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_set, \n\u001b[0;32m     19\u001b[0m                                                batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[0;32m     20\u001b[0m                                                num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_split' is not defined"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i], self.data[i]['Energy']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.keys())\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "data_train, data_test = train_split(data, 0.2, True)\n",
    "\n",
    "\n",
    "train_set = Dataset(data=data_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, \n",
    "                                               batch_size=1, \n",
    "                                               num_workers=2)\n",
    "\n",
    "test_set = Dataset(data=data_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, \n",
    "                                              batch_size=1, \n",
    "                                              num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available else torch.device('cpu')\n",
    "\n",
    "model = NN_8_256().to(device)\n",
    "model.load_state_dict(torch.load('predoptimized_2.param'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, test_loader, n_epochs=20):\n",
    "    train_loss_mae = []\n",
    "    test_loss_mae = []\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(epoch+1)\n",
    "        # train\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_loader)\n",
    "\n",
    "\n",
    "        train_mae_losses_per_epoch = []\n",
    "        for X_batch, y_batch in progress_bar:\n",
    "            X_batch_grid, y_batch = X_batch['Grid'].to(device), y_batch.to(device)\n",
    "            predictions = np.exp1m(model(X_batch_grid))\n",
    "            reaction_energy = calculate_reaction_energy(X_batch, predictions)\n",
    "            loss = criterion(np.array(reaction_energy), np.array(y_batch))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_mae_losses_per_epoch.append(loss.item())\n",
    "        train_loss_mae.append(np.mean(train_mae_losses_per_epoch))\n",
    "\n",
    "        #test\n",
    "        model.eval()\n",
    "        test_mae_losses_per_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch_grid, y_batch = X_batch['Grid'].to(device), y_batch.to(device)\n",
    "                preds = np.exp1m(model(X_batch_grid))\n",
    "                reaction_energy = calculate_reaction_energy(X_batch, predictions)\n",
    "                loss = criterion(np.array(reaction_energy), np.array(y_batch))\n",
    "                test_mae_losses_per_epoch.append(loss.item())\n",
    "        test_loss_mae.append(np.mean(test_mae_losses_per_epoch))\n",
    "        print(f'train MAE Loss = {train_loss_mae[epoch]:.8f}')\n",
    "        print(f'test MAE Loss = {test_loss_mae[epoch]:.8f}')\n",
    "\n",
    "    return train_loss_mse, train_loss_mae, test_loss_mse, test_loss_mae, preds[0].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "train_loss_mse, train_loss_mae, test_loss_mse, test_loss_mae, preds = train(model, criterion, optimizer, \n",
    "                                                                            train_dataloader,\n",
    "                                                                            test_dataloader, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5e5bb8c7f59d999df07168cdddfd96f8fceb1d4deaee65f2787e1aa74655cd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
