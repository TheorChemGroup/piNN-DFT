{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0262a903-f1e3-4fed-bb5c-0c8d2086c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np    \n",
    "import torch\n",
    "import gc\n",
    "import csv\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from NN_models import NN_2_256, NN_8_256, NN_8_64\n",
    "\n",
    "\n",
    "def ref(x, y):\n",
    "    ''' \n",
    "    returns reference energies for points of a reaction grid from Reference_data.csv\n",
    "    '''\n",
    "    hartree2kcal = 627.5095\n",
    "    with open(\"Reference_data.csv\", newline='', encoding='cp1251') as csvfile:\n",
    "        ref_file = csv.reader(csvfile, delimiter=\",\")\n",
    "        k = 1\n",
    "        if y == 391:\n",
    "            k = hartree2kcal\n",
    "        ref = []\n",
    "        for n, i in enumerate(ref_file):\n",
    "            if x <= n + 1 <= y:\n",
    "                ref.append((i[0], float(i[2]) * k))\n",
    "\n",
    "        return ref\n",
    "\n",
    "def load_ref_energies():\n",
    "    '''Returns {db_name: [equation, energy]}'''\n",
    "    ref_e = { # Получение референсных энергий\n",
    "        \"MGAE109\":ref(8, 116),\n",
    "        \"IP13\":ref(155, 167),\n",
    "        \"EA13\":ref(180, 192),\n",
    "        \"PA8\":ref(195, 202),\n",
    "        \"DBH76\":ref(251, 288) + ref(291, 328),\n",
    "        \"NCCE31\":ref(331, 361),\n",
    "        \"ABDE4\":ref(206, 209),\n",
    "        # \"AE17\":ref(375, 391),\n",
    "        \"pTC13\":ref(232, 234) + ref(237, 241) + ref(244, 248)\n",
    "        } \n",
    "    return ref_e\n",
    "\n",
    "def load_component_names():\n",
    "    '''\n",
    "    Returns {db_name: {id: {'Components': [...], 'Coefficients: [...]'\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "     which is a dictionary with Components and Coefficients data about all reactions\n",
    "    '''\n",
    "    with open(\"total_dataframe_sorted_final.csv\", newline='', encoding='cp1251') as csvfile:\n",
    "        ref_file = csv.reader(csvfile, delimiter=\",\")\n",
    "        ref = dict()\n",
    "        current_database = None\n",
    "        \n",
    "        for n, line in enumerate(ref_file):\n",
    "            line = np.array(line)\n",
    "            if n == 0:\n",
    "                components = np.array(line)\n",
    "            else:\n",
    "                reaction_id = int(line[0])\n",
    "                reaction_database = line[1]\n",
    "                reaction_component_num = np.nonzero(list(map(float, line[2:])))[0] + 2\n",
    "                if reaction_database in ref:\n",
    "                    ref[reaction_database][reaction_id] = {'Components': components[reaction_component_num], 'Coefficients': line[reaction_component_num]}\n",
    "                else: \n",
    "                    ref[reaction_database] = {reaction_id: {'Components': components[reaction_component_num], 'Coefficients': line[reaction_component_num]}}\n",
    "        return ref\n",
    "    \n",
    "    \n",
    "def get_compounds_coefs_energy_v2(reactions, energies):\n",
    "    '''Returns {id: \n",
    "                    {'Components': [...], 'Coefficients: [...]', 'Energy: float', Database: str\n",
    "                                }\n",
    "                            }\n",
    "    which is a dictionaty from load_component_names with Energy information added\n",
    "    '''\n",
    "    data_final = dict()\n",
    "    i = 0\n",
    "    databases = load_ref_energies().keys()\n",
    "    for database in databases:\n",
    "        data = reactions[database]\n",
    "        for reaction in data:\n",
    "            data_final[i] = {'Database': database,\n",
    "                             'Components': reactions[database][reaction]['Components'], #.astype(object),\n",
    "                             'Coefficients': torch.Tensor(reactions[database][reaction]['Coefficients'].astype(np.float32)),\n",
    "                             'Energy': torch.Tensor(np.array(energies[database][reaction][1]))\n",
    "            \n",
    "        }\n",
    "            i += 1\n",
    "        \n",
    "    return data_final\n",
    "\n",
    "\n",
    "def get_h5_names(reaction):\n",
    "    '''reaction must be from the function get_compounds_coefs_energy_v2'''\n",
    "    database_match = {\n",
    "        'MGAE109': 'mgae109',\n",
    "        'IP13': 'ip13',\n",
    "        'EA13': 'ea13',\n",
    "        'PA8': 'pa8',\n",
    "        'DBH76': 'ntbh38',\n",
    "        'NCCE31': 'ncce31',\n",
    "        'ABDE4': 'abde4',\n",
    "        'AE17': 'ae17',\n",
    "        'pTC13': 'ptc13'\n",
    "    }\n",
    "    names = []\n",
    "    for elem in reaction['Components']:\n",
    "        database = database_match[reaction['Database']]\n",
    "        names.append(f'{elem}.h5')\n",
    "    return names\n",
    "\n",
    "\n",
    "def add_reaction_info_from_h5(reaction):\n",
    "    '''\n",
    "    reaction must be from get_compounds_coefs_energy_v2\n",
    "    returns merged descriptos array X, integration weights, \n",
    "    a and b densities and indexes for backsplitting\n",
    "    \n",
    "    Adds the following information to the reaction dict using h5 files from the dataset:\n",
    "    Grid : np.array with grid descriptors\n",
    "    Weights : list with integration weights of grid points\n",
    "    Densities : np.array with alpha and beta densities data for grid points\n",
    "    HF_energies : list of Total HF energy (T+V) which needs to be added to E_xc\n",
    "    backsplit_ind: list of indexes where we concatenate molecules' grids\n",
    "    '''\n",
    "    X = np.array([])\n",
    "    backsplit_ind = []\n",
    "    HF_energies = np.array([])\n",
    "    for component_filename in get_h5_names(reaction):\n",
    "        with h5py.File(f'data/{component_filename}', \"r\") as f:\n",
    "            HF_energies = np.append(HF_energies, f[\"ener\"][:][0])\n",
    "            X_raw = np.array(f[\"grid\"][:])\n",
    "            if len(X) == 0:\n",
    "                X = X_raw[:, 3:-1]\n",
    "            else:\n",
    "                X = np.vstack((X, X_raw[:, 3:-1]))\n",
    "            backsplit_ind.append(len(X))\n",
    "    densities = X[:, 1:3]\n",
    "    weights = X[:,0]\n",
    "    X = X[:, 1:]\n",
    "    # sigma_a_b to norm_grad=sigma_a + sigma_b + 2*sigma_a_b\n",
    "    X[:, 3] = X[:, 2] + X[:, 4] + 2*X[:, 3]\n",
    "    # log grid data\n",
    "    eps = 10**(-12)\n",
    "    X = np.log(X+eps)\n",
    "    \n",
    "    labels = ['Grid', 'Weights', 'Densities', 'HF_energies', 'backsplit_ind']\n",
    "    values = [X, weights, densities, HF_energies, backsplit_ind]\n",
    "    for label, value in zip(labels, values):\n",
    "        reaction[label] = torch.Tensor(value)\n",
    "\n",
    "    return reaction\n",
    "\n",
    "\n",
    "def make_reactions_dict():\n",
    "    '''\n",
    "    Returns a dict like {reaction_id: {*reaction info}} with all info available listed below:\n",
    "    ['Database', 'Components', 'Coefficients', 'Energy', 'Grid', 'Weights', 'Densities', 'HF_energies', 'backsplit_ind']\n",
    "    '''\n",
    "    data = get_compounds_coefs_energy_v2(load_component_names(), load_ref_energies())\n",
    "    for i in data.keys():\n",
    "        data[i] = add_reaction_info_from_h5(data[i])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315b9b25-d148-4e66-828d-d4fa056df99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 5.37 s, total: 15.9 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = make_reactions_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbf693f-e977-4bbd-983d-543a94ec2ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Database': 'MGAE109',\n",
       " 'Components': array(['C_mgae109', 'H_mgae109', 'CH_mgae109'], dtype='<U20'),\n",
       " 'Coefficients': tensor([ 1.,  1., -1.]),\n",
       " 'Energy': tensor(84.2300),\n",
       " 'Grid': tensor([[  4.1464,   4.1465,   0.6599,  ...,   0.6602,   2.0153,  -5.5657],\n",
       "         [  4.1464,   4.1465,   5.0959,  ...,   5.0962,   2.0569,  -1.1297],\n",
       "         [  4.1463,   4.1465,   7.6894,  ...,   7.6897,   2.4699,   1.4638],\n",
       "         ...,\n",
       "         [ -7.6264,  -8.0338, -13.8937,  ..., -14.5039,  -8.2339,  -8.5278],\n",
       "         [ -7.3618,  -7.7324, -13.3484,  ..., -13.8880,  -7.9495,  -8.2130],\n",
       "         [ -7.6264,  -8.0338, -13.8937,  ..., -14.5039,  -8.2339,  -8.5278]]),\n",
       " 'Weights': tensor([2.5717e-17, 9.9780e-15, 3.2611e-13,  ..., 2.6018e-02, 2.2653e-02,\n",
       "         2.6018e-02]),\n",
       " 'Densities': tensor([[6.3204e+01, 6.3213e+01],\n",
       "         [6.3204e+01, 6.3213e+01],\n",
       "         [6.3201e+01, 6.3210e+01],\n",
       "         ...,\n",
       "         [4.8742e-04, 3.2433e-04],\n",
       "         [6.3506e-04, 4.3839e-04],\n",
       "         [4.8742e-04, 3.2433e-04]]),\n",
       " 'HF_energies': tensor([-32.6318,  -0.1908, -32.8681]),\n",
       " 'backsplit_ind': tensor([ 31098.,  64932., 129864.])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a25894b-9004-4d97-a139-16ae4137ac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88862460, 7])\n"
     ]
    }
   ],
   "source": [
    "y_single = [0.0310907, 0.01554535, \n",
    "            3.72744,   7.06042,\n",
    "            12.9352,   18.0578,\n",
    "            -0.10498,  -0.32500,\n",
    "            0.0310907,  0.01554535,  -1/(6*np.pi**2),\n",
    "            13.0720,    20.1231,      1.06835,\n",
    "            42.7198,   101.578,      11.4813,\n",
    "            -0.409286,  -0.743294,   -0.228344,\n",
    "            1]\n",
    "\n",
    "nconstants = len(y_single)\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available else torch.device('cpu')\n",
    "\n",
    "\n",
    "lst = []\n",
    "for i in range(len(data)):\n",
    "    lst.append(data[i]['Grid'])\n",
    "    \n",
    "all_grid_data = torch.cat(lst)\n",
    "print(all_grid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2250a27-9bc6-4c94-b9bf-a78e073d7bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 1.73 s, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stdscaler = StandardScaler()\n",
    "stdscaler.fit(np.array(all_grid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6178e2f-e6a3-4717-b66e-a60cebd7f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 858 ms, total: 3.88 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(data)):\n",
    "    data[i]['Grid'] = torch.Tensor(stdscaler.transform(data[i]['Grid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6289422-7d9f-485d-9a8d-7fe1fa6280db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6719,  1.5777,  1.2155,  ...,  1.2445,  1.3427,  0.3330],\n",
       "        [ 1.6719,  1.5777,  1.6793,  ...,  1.6905,  1.3486,  0.8961],\n",
       "        [ 1.6719,  1.5776,  1.9504,  ...,  1.9512,  1.4078,  1.2254],\n",
       "        ...,\n",
       "        [-0.0775, -0.0087, -0.3059,  ..., -0.2801, -0.1262, -0.0431],\n",
       "        [-0.0381,  0.0306, -0.2489,  ..., -0.2182, -0.0854, -0.0031],\n",
       "        [-0.0775, -0.0087, -0.3059,  ..., -0.2801, -0.1262, -0.0431]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['Grid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a0fe30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 287 µs, sys: 0 ns, total: 287 µs\n",
      "Wall time: 299 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        self.data[i].pop('Database', None)\n",
    "\n",
    "        return self.data[i], y_single\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.keys())\n",
    "\n",
    "\n",
    "train_set = Dataset(data=data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=None,\n",
    "                                               num_workers=1,\n",
    "                                               pin_memory=True,\n",
    "                                               shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3424dc36-9e76-41ea-bea5-ae98fe9dfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.frombuffer(X_batch['Components'][0], dtype='<U20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7eab92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.reset()\n",
    "# torch.cuda.empty_cache()\n",
    "# del X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5683c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GPUtil import showUtilization as gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3ec4ebf-9e04-4a1d-89aa-96b6862278f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, train_dataloader, n_epochs=2):\n",
    "    \n",
    "    torch.set_printoptions(precision=5)\n",
    "    \n",
    "    train_loss_mse = []\n",
    "    train_loss_mae = []\n",
    "    test_loss_mse = []\n",
    "    test_loss_mae = []\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch', epoch+1)\n",
    "        # train\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        train_mse_losses_per_epoch = []\n",
    "        train_mae_losses_per_epoch = []\n",
    "        \n",
    "        progress_bar = tqdm(train_dataloader)\n",
    "\n",
    "        # gradscaler = GradScaler()\n",
    "        for i, (X_batch, y_batch) in enumerate(progress_bar):\n",
    "            # print(np.frombuffer(X_batch['Components'][0], dtype='<U20'), X_batch['Grid'][0].shape)\n",
    "\n",
    "            X_batch = X_batch['Grid'].to(device, non_blocking=True)\n",
    "            y_batch = torch.tile(torch.Tensor(y_batch), [X_batch.shape[0],1]).to(device, non_blocking=True)\n",
    "            # with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            MAE = mean_absolute_error(predictions.cpu().detach(), y_batch.cpu().detach())\n",
    "            MSE = loss.item()\n",
    "            train_mse_losses_per_epoch.append(MSE)\n",
    "            train_mae_losses_per_epoch.append(MAE)\n",
    "            # scheduler.step(MAE)\n",
    "            progress_bar.set_postfix(MAE = MAE, MSE = MSE)\n",
    "\n",
    "            pred = predictions[0].cpu().detach().numpy()\n",
    "            del X_batch, y_batch, predictions, loss, MAE, MSE\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        train_loss_mse.append(np.mean(train_mse_losses_per_epoch))\n",
    "        train_loss_mae.append(np.mean(train_mae_losses_per_epoch))\n",
    "        \n",
    "        print(f'train MSE Loss = {train_loss_mse[epoch]:.8f}')\n",
    "        print(f'train MAE Loss = {train_loss_mae[epoch]:.8f}')\n",
    "        \n",
    "    return train_loss_mse, train_loss_mae, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "902e7e22-8565-4d14-b156-42d54bc93a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a4a42f6-a9ba-49da-8b6e-7098c1e254c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_2_256(DFT='SVWN').to(device)\n",
    "# model.load_state_dict(torch.load('predopt/predopt_8_256.param', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01242e9c-f915-4f7d-ab43-c5a81a71f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8897c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 400, 700, 1000], gamma=1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59eff639-e8f9-44b8-bfa8-57a763eb049d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss_mse, train_loss_mae, preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(model, criterion, optimizer, \n\u001b[1;32m      2\u001b[0m                                               scheduler, train_dataloader, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_loss_mse, train_loss_mae)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted coef\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss_mse, train_loss_mae, preds = train(model, criterion, optimizer, \n",
    "                                              scheduler, train_dataloader, n_epochs=1)\n",
    "\n",
    "print(train_loss_mse, train_loss_mae)\n",
    "print('predicted coef', '\\n', preds)\n",
    "print('exact coef', '\\n', np.array(y_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90def969-43df-4bfe-9e43-aebde816dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'predopt/predopt_8_64_std_log.param')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fd0f4-6e09-4d70-9bb0-f7dd1950a0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.11 [python-pytorch1_11]",
   "language": "python",
   "name": "conda-env-python-pytorch1_11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1be0b9c33878e6644d545b09300b0d1acdcb9b85e0a2f17cbe61dcb3e9601fd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
